{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "\n",
    "\n",
    "Neighbors-based classification is a type of *instance-based learning* or *lazy learning*: it does not attempt to construct a general internal model (using a boundary) but simply stores instances of the training data and defers the decision to generalize until a new test instance in encountered. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\n",
    "<img src=\"./KNN_neighbors.png\"  width=50% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries\n",
    "\n",
    "Before we import the data, let's load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "In the previous lecture, we looked at the data on customer churn for a telecom operator. \n",
    "- We will reload the same dataset into a `DataFrame` named **\"df\"** using once more the function  `.read_csv()` ([Check the pandas read_csv() documentation if needed](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)). \n",
    "- To get acquainted with our data, let’s look at the first 5 entries using `.head()`\n",
    "- Check the dimensionality of the data using `.shape`\n",
    "- The dataset is provided in your Lab folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the data from the telecom_churn.csv file (provided to you in the Lab folder) \n",
    "# using the pd.read_csv() function from pandas into a variable named \"df\" \n",
    "# 2. Print the dimensionality (shape) of the df DataFrame\n",
    "# 3. Preview the first 5 rows of df using the function .head()\n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question \n",
    "\n",
    "Can you spot the target variable we are trying to predict? \n",
    "\n",
    "### Split to input variables (X) and class vector (y)\n",
    "\n",
    "We first start by splitting our dataset into the `X` matrix (input features - independent variable) and `y` vector (class target - dependent variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most common step across all Supervised Machine Learning models in Python\n",
    "\n",
    "# Store the input feature data into a new variable named \"X\": Extract all columns **except** from \n",
    "# the target label column ('Churn') using either indexing (.loc / .iloc) or, easier, df.drop() with axis=1\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Store the target data (label/class column) into a new variable named \"y\"\n",
    "# Keep only the label (class) column 'Churn' from df \n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Sanity check: print the dimensions (using \".shape\") for both X and y \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for ease throughout today's notebook activity, let's work only with the numerical features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep only the numerical columns for today's notebook for simplicity \n",
    "\n",
    "X = X.select_dtypes(include=np.number)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the class frequencies\n",
    "\n",
    "An important aspect to understand before applying any classification algorithm is how the output labels are distributed. Are they evenly distributed or not? Imbalances in distribution of labels can often lead to poor classification results for the minority class even if the classification results for the majority class are very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to check the counts per class for the target variable, use the function .value_counts() on y \n",
    "# By default, the entries in the output are sorted from the most to the least frequently-occurring values.\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: what do you observe??\n",
    "\n",
    "In our case, the data are **imbalanced**; that is, our two target classes, churned and non-churned customers, are not represented equally in the dataset. As we will see in the following courses, this fact may imply some restrictions on measuring the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization - class frequencies\n",
    "The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the `seaborn`'s function `countplot()`. There is another function in `seaborn` that is somewhat confusingly called `barplot()` and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature. \n",
    "\n",
    "Let's plot the distributions for categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sns.countplot() function to plot the feature \"Churn\" (argument x=\"Churn\") directly on the data df\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above vividly illustrates the **imbalance** in our target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping (encoding) the categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the class variable to be in machine-readable form and ready to be used by ML models, it needs to be encoded in a numerical format. `LabelEncoder` from `sklearn` can be used to encode target labels with value between `0` and `n_classes-1`. **This transformer should be used to encode ONLY target values (y variable), and not the input features in X** (in which case, we can use One Hot Encoding or other ways of encoding). \n",
    "\n",
    "Read more about [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) and [Transforming the prediction variable(y)](https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Convert the categorical values within y into numbers using the LabelEncoder() from sklearn\n",
    "# The values will be converted from True / False to integers 1 and 0.\n",
    "# 1. Instantiate a LabelEncoder object and store into a variable named \"le\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "# 2. The .fit_transform() function takes a categorical column and converts/maps it into numerical values.\n",
    "# Call the .fit_transform() function from the \"le\" LabelEncoder variable created above passing the y data \n",
    "# Assign the returned encoded labels back to y to overwrite its entries. \n",
    "# Display the updated entries of y; have they changed?  \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split (Holdout validation)\n",
    "\n",
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split our dataset into two disjoint sets: train and test (Holdout method).\n",
    "\n",
    "Use `sklearn`’s `train_test_split()` function to randomly split the data into train and test sets (visit the [train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and the  [model cross-validation documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)). Typically, 20% test size seems reasonable enough for checking the performance of models.\n",
    "\n",
    "- **Note: it’s good practice to split the train and test sets before doing any feature engineering and/or scaling to avoid data leakage!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split() function and pass the following arguments: \n",
    "# (1) the X matrix (2) the y vector (3) test_size=0.2 (4) random_state=1 (for result reproducibility)\n",
    "# Store the results into the new variables X_train, X_test, y_train, y_test (simultaneously)\n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Print the shape of X_train, X_test, y_train, y_test \n",
    "# Do the X and y variables for train and test respectively match? \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you observe when running the code below? \n",
    "\n",
    "plt.figure(figsize=(17,7))\n",
    "ax = sns.boxplot(data= X_train)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to feeding our data to any other supervised or unsupervised learning technique, we always need to **scale** our data. For this, we will subtract the mean from each variable and divide it by its standard deviation. All of this can be done with `StandardScaler`.\n",
    "\n",
    "**NOTE: We should only fit the scaler on the train set and use it to transform the test set to avoid data leakage.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the StandardScaler() from sklearn and store into a variable named \"scaler\" \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "# Fit the scaler on the X_train data and transform the data - Use the scaler.fit_transform() on the X_train set \n",
    "# (alternatively, we can .fit() and .transform() in two separate steps). Remember, we fit ONLY on the train data. \n",
    "# Assign the result back to the variable X_train\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Transform (do not fit!) the test data (X_test) using the fitted scaler named \"scaler\". Use the scaler.transform()\n",
    "# function on the X_test. Assign the result back to the variable X_test\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: scaling in sklearn converts the X_train into a numpy array (stripping away all column/index names) \n",
    "# We may often need to maintain the names of the columns for further analysis e.g. to further use in PCA \n",
    "# It is therefore useful to leverage the functionality provided by pandas (casting/creating DFs)\n",
    "# There are various ways to accomplish this at this scaling step, one of which could be the following: \n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=scaler.feature_names_in_) \n",
    "X_test  = pd.DataFrame(X_test,  columns=scaler.feature_names_in_) \n",
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you observe when running the code below after scaling? \n",
    "\n",
    "plt.figure(figsize=(17,7))\n",
    "ax = sns.boxplot(data= X_train)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Classification\n",
    "\n",
    "For every classification model built with `scikit-learn`, we will follow four main steps:\n",
    "\n",
    "1. Building the classification model (using either default, pre-defined or optimized parameters)\n",
    "2. Training the model\n",
    "3. Testing the model\n",
    "4. Performance evaluation using various metrics.\n",
    "\n",
    "### K Nearest Neighbors\n",
    "\n",
    "\n",
    "Neighbors-based classification is a type of *instance-based learning* or *lazy learning*: it does not attempt to construct a general internal model (using a boundary), but simply stores instances of the training data and defers the decision to generalize until a new test instance in encountered. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\n",
    "\n",
    "#### Train, optimise and test a KNN algorithm with scikit-learn\n",
    "\n",
    "`scikit-learn` implements two different nearest neighbors classifiers: \n",
    "- `KNeighborsClassifier` implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user. \n",
    "- `RadiusNeighborsClassifier` implements learning based on the number of neighbors within a fixed radius of each training point, where is a floating-point value specified by the user. \n",
    "The k-neighbors classification in `KNeighborsClassifier` is the most commonly used technique.\n",
    "\n",
    "The `KNeighborsClassifier` object allows you to set the value of K using the `n_neighbors` parameter (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN using the pre-processed data (without PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the KNeighborsClassifier() classifier using its default parameters \n",
    "# (default parameters: do *NOT* pass any parameter values in KNeighborsClassifier()) \n",
    "# Assign the result into a new variable named \"knn\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Step 2 - Fit the knn model to the training set (use knn.fit())\n",
    "# Pass as function arguments the train matrix X_train and the class vec y_train \n",
    "# No need to assign it into a new variable\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Step 3 - Predict the test data using the knn model (use knn.predict())\n",
    "# Pass as argument ONLY the test matrix X_test\n",
    "# Save the prediction output into a new variable \"y_pred\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Step 4 - Print the final overall accuracy for the test set using metrics.accuracy_score()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion_matrix for the test set using metrics.confusion_matrix()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification_report for the test set using metrics.classification_report()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with PCA\n",
    "\n",
    "Most real-world datasets have many features, sometimes, many thousands of them. Each of them can be considered as a dimension in the space of data points. Consequently, more often than not, we deal with high-dimensional datasets, where entire visualization is quite hard.\n",
    "\n",
    "To look at a dataset as a whole, we need to decrease the number of dimensions used in visualization without losing much information about the data. This task is called *dimensionality reduction* and is an example of an *unsupervised learning* problem because we need to derive new, low-dimensional features from the data itself, without any supervised input.\n",
    "\n",
    "One of the well-known dimensionality reduction methods is *Principal Component Analysis* (PCA). Its limitation is that it is a *linear* algorithm that implies certain restrictions on the data. There are also many non-linear methods, collectively called *Manifold Learning*. One of the best-known of them is *t-SNE*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the PCA library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating a PCA object\n",
    "\n",
    "In scikit-learn, PCA is implemented as a transformer object that learns `n` components in its `fit()` method, and can be used on new data to project it on these components. More information on how to use the `pca()` function and its parameters can be found at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Remember: Prior to feeding our data to PCA or any other supervised or unsupervised learning technique, we need to **scale** our data. For this, we will subtract the mean from each variable and divide it by its standard deviation. All of this can be done with `StandardScaler` or `MinMaxScaler()`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise/instantiate PCA() into a new variable named \"pca\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature name for each Principal Component (such as PC1, PC2, ...) up to the number of current dimensions\n",
    "# and store in a list for further use \n",
    "\n",
    "PCs = ['PC'+str(i+1) for i in range(X_train.shape[1])]\n",
    "print(PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA scores\n",
    "\n",
    "The values of the Principal Components (scores) can be computed by the `fit_transform()` function (alternatively, `fit()` followed by `transform()` in two separate steps) . This function returns a matrix with the principal components, where the first column in the matrix contains the first principal component, the second column the second component, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PCA scores matrix for the training data: remember we are fitting ONLY on the training set \n",
    "# Call from the pca object that was created above the .fit_transform() function passing the scaled X_train data. \n",
    "# (alternatively, instead of .fit_transform(), we can do .fit() and .transform() in two separate steps)\n",
    "# Save the result of the .fit_transform() into a new variable \"X_train_scores\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with previous examples, .fit_transform() converts the data into a numpy array. Therefore, we can convert/cast  \n",
    "# it back into a DataFrame using the pandas.DataFrame() constructor as follows. The purpose here is to have the \n",
    "# column names equal to the new Principal Components \n",
    "\n",
    "X_train_scores = pd.DataFrame(X_train_scores, columns = PCs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensionality of the PCA scores and preview X_train_scores \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ##########  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you get the PC scores for the test data as above? \n",
    "# NOTE we are fitting ONLY on the training set and only transforming on the test set \n",
    "\n",
    "######### FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained variance and cumulative variance\n",
    "\n",
    "But how much information have we lost? We can figure this out by looking at the explained and cumulative variance. The explained variance gives us the proportion of variance explained by each successive Principal Component. The cumulative variance is obtained by adding the successive proportions of explained variance to obtain the total sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the explained variance\n",
    "exp_var = [i*100 for i in pca.explained_variance_ratio_]\n",
    "\n",
    "# Calculate the cumulative variance\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_*100)\n",
    "\n",
    "# Combine both in a data frame\n",
    "pca_var = pd.DataFrame(data={'exp_var': exp_var, 'cum_var': cum_var}, index=PCs)\n",
    "pca_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this information to determine a threshold on how many Principal Components we should ideally keep. \n",
    "\n",
    "We can also plot the explained variance using a barplot with seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative variance per PC using a barplot\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = sns.barplot(x=pca_var.index, y='cum_var', data=pca_var)\n",
    "ax.set(xlabel='Principal Components', ylabel='Cumulative Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for plotting & EDA \n",
    "#### Plot the PCA scores in a scatterplot (enhanced with a color for each different class of Churn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the class label to be used in the scatterplot for ease when using seaborn\n",
    "\n",
    "X_train_scores_enhanced = X_train_scores.join(df['Churn'])\n",
    "X_train_scores_enhanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an enhanced scatterplot of the first two Principal Components\n",
    "# Remember, we are using scores (!) rather than df as input at this stage\n",
    "\n",
    "ax = sns.lmplot(x='PC1',y='PC2', data= X_train_scores_enhanced, hue='Churn', \n",
    "                markers=['o', 'x'], palette=['orange', 'blue'], fit_reg=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Supervised Learning \n",
    "\n",
    "Passing the PCA scores into a classifier such as KNN \n",
    "\n",
    "#### KNN using the pre-processed data with PCA\n",
    "\n",
    "Can you repeat KNN only this time round using the pre-processed data from PCA? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on the cut-off for the number of Principal Components you would like to keep\n",
    "# e.g. how many PCs do we need to reach e.g. 95% or 99% of the variance? \n",
    "# Check the cumulative variance table/plot above to decide on the number of PCs \n",
    "# Filter the columns of X_train_scores and X_test_scores accordingly using .iloc[]\n",
    "# Conduct a sanity check on the dimensionality of the matrices \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the KNeighborsClassifier() classifier using its default parameters \n",
    "# default parameters: do *NOT* pass any parameter values in KNeighborsClassifier()\n",
    "# Assign the result into a new variable named \"knn\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "# Step 2 - Fit the knn model to the training set (use knn.fit())\n",
    "# Pass as function arguments the train matrix X_train_scores and the class vec y_train \n",
    "# No need to assign it into a new variable\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Step 3 - Predict the test data using the knn model (use knn.predict())\n",
    "# Pass as argument only the test matrix X_test_scores\n",
    "# Save the prediction output into a new variable \"y_pred\"\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "# Step 4 - Print the final overall accuracy for the test set using metrics.accuracy_score()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion_matrix for the test set using metrics.confusion_matrix()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification_report for the test set using metrics.classification_report()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What do we observe? \n",
    "\n",
    "We observe that we accomplish a similar accuracy to the one we saw before while using less features in our modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
