{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing & exploratory data analysis (EDA) \n",
    "\n",
    "### Adapted from [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "\n",
    "Author: Yury Kashnitsky. Translated and edited by Christina Butsko, Yuanyuan Pao, Anastasia Manokhina, Sergey Isaev and Artem Trunov. This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose.\n",
    "\n",
    "### Data pre-processing \n",
    "\n",
    "Data pre-processing is the process of transforming the raw data into a clean data set. Raw, real-world data is messy. Not only may it contain errors and inconsistencies, but it is often incomplete, and doesn’t have a regular, uniform design. The dataset is pre-processed in order to check and fix missing values, noisy data, and other inconsistencies before feeding it into a ML algorithm. The goal of data pre-processing is to improve the quality of the data and to make it more suitable for the specific data mining task.\n",
    "\n",
    "### Visualizations\n",
    "\n",
    "In the field of Machine Learning, *data visualization* is not just making fancy graphics for reports; it is used extensively in day-to-day work for all phases of a project.\n",
    "\n",
    "To start with, visual exploration of data is the first thing one tends to do when dealing with a new task. We do preliminary checks and analysis using graphics and tables to summarize the data and leave out the less important details. It is much more convenient for us, humans, to grasp the main points this way than by reading many lines of raw data. It is amazing how much insight can be gained from seemingly simple charts created with available visualization tools. Next, when we analyze the performance of a model or report results, we also often use charts and images. Sometimes, for interpreting a complex model, we need to project high-dimensional spaces onto more visually intelligible 2D or 3D figures.\n",
    "\n",
    "All in all, visualization is a relatively fast way to learn something new about your data. Thus, it is vital to learn its most useful techniques and make them part of your everyday ML toolbox. In this notebook, we are going to get hands-on experience with visual exploration of data using popular libraries such as `pandas`, `matplotlib` and `seaborn`.\n",
    "\n",
    "\n",
    "<img src=\"visual.png\"  width=50% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Libraries \n",
    "\n",
    "First we need to load the required Python libraries. Libraries are like extensions to the base python that add functionality or help to make tasks more convenient to do. We will load some libraries that will boost your data handling capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "print(\"libraries all imported, ready to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In Jupyter notebooks, Pandas DataFrames are printed as these pretty tables while `print(df.head())` is less nicely formatted. By default, Pandas displays 20 columns and 60 rows, so, if your DataFrame is bigger, use the `pd.set_option()` function as shown in the example above:\n",
    "\n",
    "```python\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data \n",
    "\n",
    "We’ll demonstrate the main methods in action by analyzing a [dataset](https://www.kaggle.com/code/kashnitsky/topic-1-exploratory-data-analysis-with-pandas/data?select=telecom_churn.csv) on the churn rate of telecom operator clients. \n",
    "\n",
    "- As a first step, we load the dataset with pandas. To achieve this you will use the `read_csv()` method. We just need to point to the location of the dataset and indicate under what name we want to store the data,  and pandas will do the rest. \n",
    "\n",
    "- For consistency, use the name **df** to store the output of `read_csv()` . At a first stage, the data has only been loaded. Let's have a look at the top few lines - we can use the `.head()` method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data using the .read_csv() function from pandas (pd) into a DataFrame named \"df\" \n",
    "# Print out the first 5 rows of the DF using the function .head()\n",
    "# Try printing more rows of the DF (e.g. 10 or 20) using the function head() to get a sense of the data\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect the target/class (dependent) variable y: \n",
    "\n",
    "Note: The last data column, **Churn**, is our target variable. It is binary: *True* indicates that that the company eventually lost this customer, and *False* indicates that the customer was retained. Later, we will build models that predict this feature based on the remaining features. This is why we call it a **target**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data dimensionality \n",
    "Let’s have a look at data dimensionality using `shape` (Note: `.shape` does not take any parentheses!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensionality of the DataFrame df \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the table contains 3333 rows and 20 columns.\n",
    "\n",
    "Now let’s try printing out the column names using `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of the DataFrame df \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `info()` method to output some general information about the dataframe (including the type of data stucture, column data types and number of non-null values per column, among others): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the info() function on the DataFrame df\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values   \n",
    "\n",
    "We can see from the results above that there are some missing values in our DataFrame in the feature \"International plan\". As we have seen, Pandas treats None and NaN as essentially interchangeable for indicating missing or null values. To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures. They are:\n",
    "\n",
    "- `isnull()`: Generate a boolean mask indicating missing values\n",
    "- `notnull()`: Opposite of `isnull()`\n",
    "- `dropna()`: Return a filtered version of the data\n",
    "- `fillna()`: Return a copy of the data with missing values filled or imputed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts of missing values for each column in the \"df\" DataFrame\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates\n",
    "\n",
    "A dataset may include data objects that are duplicates or almost duplicates of one another. You can use the `.drop_duplicates()` function to return the deduplicated data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates (if the exist) either inplace=True OR with assignment (assign back to df to overwrite the results) \n",
    "# Extra sanity check: print the shape of df before and after the drop.\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to input variables and class vector\n",
    "\n",
    "We first start by setting the X matrix (input features) and y vector (class target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most common step across all Supervised Machine Learning models in Python\n",
    "\n",
    "# Store the feature data into a new variable named \"X\" - Extract all columns **except** \n",
    "# from the label column ('Churn') using either indexing (.loc / .iloc) or, easier, df.drop() with axis=1\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Store the target data (label/class column) into a new variable named \"y\"\n",
    "# Get only the label (class) column 'Churn' from df \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the dimensions (using \"shape\") for both X and y \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate the class frequencies\n",
    "An important aspect to understand before applying any classification algorithm is how the output labels are distributed. Are they evenly distributed or not? Imbalances in distribution of labels can often lead to poor classification results for the minority class even if the classification results for the majority class are very good.\n",
    "\n",
    "##### 1. Tabular format - frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to check the distribution of the binary class, use either the function .value_counts() on y \n",
    "# or .groupby() to get the counts per class for the target variable \n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2850 users out of 3333 are *loyal* as their `Churn` value is False. To calculate fractions, pass `normalize=True` to the `value_counts()` function. By default, the entries in the output are sorted from the most to the least frequently-occurring values.\n",
    "\n",
    "##### What do you observe??\n",
    "In our case, the data are **imbalanced**; that is, our two target classes, churned and non-churned customers, are not represented equally in the dataset. As we will see in the following courses, this fact may imply some restrictions on measuring the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Visualization - class frequencies\n",
    "The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the `seaborn`'s function `countplot()`. There is another function in `seaborn` that is somewhat confusingly called `barplot()` and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature. \n",
    "\n",
    "Let's plot the distributions for categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sns.countplot() function from seaborn to plot the feature \"Churn\" (argument x=\"Churn\") from df\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above vividly illustrates the **imbalance** in our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapping (encoding) the categorical variable\n",
    "\n",
    "In order for the class variable to be in machine-readable form and ready to be used by ML models, it needs to be encoded in a numerical format. `LabelEncoder` from `sklearn` can be used to encode target labels with value between `0` and `n_classes-1`. **This transformer should be used to encode ONLY target values (y variable), and not the input features in X** (in which case, we can use One Hot Encoding or other ways of encoding). \n",
    "\n",
    "Read more about [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) and [Transforming the prediction variable(y)](https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical values within y into numbers using the LabelEncoder() from sklearn\n",
    "# The values will be converted from True / False to integers 1 and 0.\n",
    "\n",
    "\n",
    "# Initialise a LabelEncoder object and store into a variable named \"le\"\n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Call the .fit_transform() function from the \"le\" label encoder object created above on the y data \n",
    "# and assign the returned encoded labels back to y to overwrite its entries. \n",
    "\n",
    "# The .fit_transform() function takes a categorical column and converts/maps it to numerical values.\n",
    "# Display the entries of y (after the application of LabelEncoder) \n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Multivariate visualization\n",
    "\n",
    "*Multivariate* plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.\n",
    "\n",
    "\n",
    "##### Correlation matrix\n",
    "\n",
    "Let's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n",
    "\n",
    "First, we will use the method [`corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) on a `DataFrame` that calculates the correlation between each pair of features. Then, we pass the resulting *correlation matrix* to [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from `seaborn`, which renders a color-coded matrix for the provided values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix of the numerical_features from the X DataFrame using .select_dtypes('number').corr()\n",
    "# and save it in a variable called 'corr_matrix' \n",
    "# Then pass the corr_matrix to the sns.heatmap() function for plotting. \n",
    "\n",
    "# Optional: you can use f, ax = plt.subplots(figsize=(14, 8))  \n",
    "# Optional sns.heatmap arguments : annot=True,  annot_kws={'size': 8} and cmap=\"Spectral_r\" \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the colored correlation matrix generated above, we can see that there are 4 variables such as *Total day charge* that have been calculated directly from the number of minutes spent on phone calls (*Total day minutes*). \n",
    "\n",
    "##### Scatter plot\n",
    "\n",
    "The *scatter plot* displays values of two numerical variables as *Cartesian coordinates* in 2D space. Scatter plots in 3D are also possible. Let's try out the function [`scatter()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) from the `matplotlib` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plt.scatter() function from matplotlib to plot the features \"Total day minutes\" \n",
    "# and \"Total day minutes\" from the dataframe X against each other in a scatterplot. Also add \"s=2\" for the size. \n",
    "# Additionally, you can use the plt.xlabel() and plt.ylabel() functions to set the axes' labels\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellipse-like shape is aligned with the axes.\n",
    "\n",
    "There is a slightly fancier option to create a scatter plot with the `seaborn` library. The function [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) plots two histograms that may be useful in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .jointplot() function from seaborn to plot \"Total day minutes\" vs. \"Total night minutes\"\n",
    "# from the X dataframe with kind=\"scatter\" and size s=12 \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatterplot matrix\n",
    "\n",
    "In some cases, we may want to plot a *scatterplot matrix* such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a bit of time to execute \n",
    "\n",
    "g = sns.PairGrid(X)\n",
    "g.map(sns.scatterplot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Categorical Features\n",
    "\n",
    "Most ML libraries and tools will only accept *numerical values* as their input. In the case where we have categorical features present, we need to represent them as numerical values. Unlike the class label, when dealing with categorical input features, typically one converts each categorical feature using “one-hot encoding”. The input in one-hot encoding is the vector of discrete categorical values, and the output will be a sparse matrix where each column corresponds to one possible value of one feature.\n",
    "\n",
    "In our example, the feature State is a categorical feature with values such as [\"KS\", \"OH\", \"NJ\", \"OK\"] etc. Such features can be easily mapped to dummy variables which could be expressed as [0, 1, 2]. Can you spot any problem though with this approach? Even though the country values do not come in any particular order, a machine learning algorithm will now assume that \"KS\" is larger than \"OH\", and \"OH\" is larger than \"NJ\", and so on. Although this assumption is incorrect, the algorithm could still produce useful results. However, those results would not be optimal.\n",
    "\n",
    "The correct approach in this case is to apply one-hot encoding. This estimator transforms each unique categorical value of a single input categorical feature to a new dummy feature. So, for our 51 unique country values , we will end up with 51 new dummy features after one-hot-encoding. There are plenty of libraries and functions that are used for one-hot encoding. \n",
    "\n",
    "In this example, we will use the `get_dummies()` function from pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function .unique() on the X['State'] column \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function .nunique() on the X['State'] column \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the categorical feature Country\n",
    "\n",
    "# Create a new variable \"states\" using just the entries of X['State'] \n",
    "# As a second step, apply pd.get_dummies(states, dtype=int) and assign back to the variable 'states' to overwrite the entries of the dataframe.  \n",
    "# Preview the first rows of states\n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we want to update in our Dataframe the categorical feature \"State\" with the result of one-hot encoding. Remember, that are going to remove a single feature, and add 51 new dummy variables created by one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove (drop) the column 'State' from the dataframe X (you may need to set the axis accordingly!)\n",
    "# Drop with inplace=True OR with assignment back to X \n",
    "\n",
    "# Use .join() on X with the result of one-hot encoding from the dataframe states you created above. \n",
    "# Assign back to X to overwrite the entries. Preview the first few rows of X \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the values \"Yes\"/\"No\" of the binary features into numerical values using [`pandas.Series.map()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting binary features. In this case, we could use either one hot encoding as above  \n",
    "# or simply use the .map() functionality shown below\n",
    "\n",
    "X[\"International plan\"] = X[\"International plan\"].map({\"Yes\": 1, \"No\": 0})\n",
    "X[\"Voice mail plan\"]    = X[\"Voice mail plan\"].map({\"Yes\": 1, \"No\": 0})\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split our dataset into two disjoint sets: train and test (Holdout method).\n",
    "\n",
    "Use `sklearn`’s `train_test_split()` function to randomly split the data into train and test sets (visit the [train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and the  [model cross-validation documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)). Typically, 20% test size seems reasonable enough for checking the performance of models.\n",
    "\n",
    "**Note: it’s good practice to split the train and test sets before doing any feature engineering and/or scaling to avoid data leakage!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .train_test_split() function from sklearn and pass the following arguments: \n",
    "# (1) the X matrix (2) the y vector (3) test_size=0.2 (4) random_state=1 (for result reproducibility)\n",
    "# Store the results into the new variables X_train, X_test, y_train, y_test (simultaneously)\n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "# Print the shape of X_train, X_test, y_train, y_test \n",
    "# Do the X and y variables for train and test respectively match? \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in the missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of missing values, we should take care to replace them in the dataset as they prevent machine learning algorithms to run. There are many functions such as `.fillna()` and `.SimpleImputer` from `sklearn` as well as **many strategies** to help with missing data and they depend on whether the missing data is **numeric** or **categorical**.  \n",
    "\n",
    "What strategy is best for you problem very much depends on the specifics of your dataset. However, generally speaking it is not worth to remove large chunks of data.\n",
    "\n",
    "- simply removing rows where there is missing data (e.g. `dropna()` can achieve this)\n",
    "- imputing the values with a summary statistic such as mean or median or most frequent value (e.g. `.fillna()` from `pandas` or `.SimpleImputer()` from `sklearn` module)\n",
    "- replace the values with a reasonable estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .unique() function on the column \"International plan\" from X_train to get the unique values\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both the X_train and X_test use the .fillna() function from pandas to fill in the empty values (NAs) \n",
    "# of the column 'International plan' with a value from the .unique() output above. \n",
    "\n",
    "# Do not forget to assign it back as the new value of that column \n",
    "# e.g. X_train['International plan'] = ..... \n",
    "# Alternatively, you can use the argument inplace=True within .fillna()\n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative solution using the `SimpleImputer()` functionality from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Alternative solution using the SimpleImputer() functionality from sklearn\n",
    "\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=1)\n",
    "\n",
    "# X_train = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "# X_test  = pd.DataFrame(imputer.transform(X_test), index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `describe` method shows basic statistical characteristics of each numerical feature (`int64` and `float64` types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the .describe() function on the X_train\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you observe when visualizing the following plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the X_train data in a seaborn boxplot \n",
    "# Optional: set the plt.figure(figsize=(18,10)) and ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler \n",
    "\n",
    "Prior to feeding our data to any other supervised or unsupervised learning technique, we need to **scale** our data. For this, we will subtract the mean from each variable and divide it by its standard deviation. All of this can be done with `StandardScaler`.\n",
    "\n",
    "**NOTE: We should only fit the scaler on the train set and use it to transform the test set to avoid data leakage.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the StandardScaler() from sklearn \n",
    "# Store into a variable named \"scaler\" \n",
    "\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "# Fit the scaler on the X_train data - Use the scaler.fit_transform() on the X_train set \n",
    "# Assign the result back to the variable X_train \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n",
    "\n",
    "\n",
    "# Transform (do not fit!) the test dataset using the (fitted) scaler - \n",
    "# Use the scaler.transform() on the X_test\n",
    "# Assign the result back to the variable X_test\n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you observe when running the code below? \n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "ax = sns.boxplot(data= X_train)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful resources\n",
    "\n",
    "* The same notebook as an interactive web-based [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas)\n",
    "* [\"Merging DataFrames with pandas\"](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/tutorials/merging_dataframes_tutorial_max_palko.ipynb) - a tutorial by Max Plako within mlcourse.ai (full list of tutorials is [here](https://mlcourse.ai/tutorials))\n",
    "* [\"Handle different dataset with dask and trying a little dask ML\"](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/tutorials/dask_objects_and_little_dask_ml_tutorial_iknyazeva.ipynb) - a tutorial by Irina Knyazeva within mlcourse.ai\n",
    "* Main course [site](https://mlcourse.ai), [course repo](https://github.com/Yorko/mlcourse.ai), and YouTube [channel](https://www.youtube.com/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n",
    "* Official Pandas [documentation](http://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "* Course materials as a [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse)\n",
    "* Medium [\"story\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-1-exploratory-data-analysis-with-pandas-de57880f1a68) based on this notebook\n",
    "* If you read Russian: an [article](https://habrahabr.ru/company/ods/blog/322626/) on Habr.com with ~ the same material. And a [lecture](https://youtu.be/dEFxoyJhm3Y) on YouTube\n",
    "* [10 minutes to pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "* [Pandas cheatsheet PDF](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)\n",
    "* GitHub repos: [Pandas exercises](https://github.com/guipsamora/pandas_exercises/) and [\"Effective Pandas\"](https://github.com/TomAugspurger/effective-pandas)\n",
    "* [scipy-lectures.org](http://www.scipy-lectures.org/index.html) — tutorials on pandas, numpy, matplotlib and scikit-learn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
