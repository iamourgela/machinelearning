{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Titanic - Machine Learning from Disaster - Predicting the Survival of Titanic Passengers\n",
    "\n",
    "\n",
    "### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import warnings \n",
    "\n",
    "# Pre-processing, model selection and tuning techniques \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Algorithms - Unsupervised Learning\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "# Algorithms - Supervised Learnin - classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Other classifiers if you would like to explore (optional) \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Kaggle’s Titanic Machine Learning Dataset – a classic open-source introduction to the realm of machine learning. The link for the competition and data is https://www.kaggle.com/competitions/titanic. The dataset has information about different passengers on the Titanic ship and their survival. We have the following features in our dataset: \n",
    "\n",
    "- survival -> Survival -> 0 = No, 1 = Yes\n",
    "- pclass -> Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "- sex -> Sex\n",
    "- Age -> Age in years\n",
    "- sibsp -> # of siblings / spouses aboard the Titanic\n",
    "- parch -> # of parents / children aboard the Titanic\n",
    "- ticket -> Ticket number\n",
    "- fare -> Passenger fare\n",
    "- cabin -> Cabin number\n",
    "- embarked -> Port of Embarkment\n",
    "\n",
    "#### Data Notes \n",
    "\n",
    "- As part of the Kaggle competition, based on the patterns you find in titanic_known.csv, you will have to predict whether the other 418 passengers on board in titanic_unknown.csv survived.\n",
    "\n",
    "**Note that titanic_unknown.csv does not have a \"Survived\" column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data in this use case are provided to you as titanic_known.csv and titanic_unknown.csv files. \n",
    "# Load both of the files into two new DataFrames titanicData and unknownData respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dimensionality and preview the known titanicData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensionality of titanicData and preview its first rows\n",
    "# Take a moment to familiarize yourselves with the entries in the dataset (and their data types) \n",
    "# Have you detected the class variable that we are trying to predict? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training-set has 891 examples and 11 features + the target variable (survived).  \n",
    "\n",
    "Get the dimensionality and preview the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensionality of unknownData and preview its first rows\n",
    "# Familiarize yourselves with the entries in the dataset (and their data types) \n",
    "\n",
    "# !! Note that titanic_unknown.csv does not have a \"Survived\" column - this information is hidden from you, \n",
    "# and how well you do at predicting these hidden values will determine how highly you score in the actual Kaggle competition! \n",
    "# (they screen the results against the hidden classes only the owners possess) \n",
    "\n",
    "# !! The unknownData set mimics a real-world scenario of new, unseen data arriving at some point through any real-world (production) model,\n",
    "# and the model's task will be to predict them using all the learning and generalization capabilities we have leveraged from our known data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "\n",
    "Note: From the previews above, we can note a few things. \n",
    "- First of all, that we need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. \n",
    "- Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. \n",
    "- We can also spot some more features, that contain missing values (NaN = not a number), that we'll need to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data types and missing values of each column \n",
    "\n",
    "- Which features are categorical?\n",
    "- Which features are numerical?\n",
    "\n",
    "#### 1) Train data\n",
    "\n",
    "2 of the features are floats, 5 are integers and 5 are objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data types or relevant info of titanicData \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values per column in titanicData  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra - optional : Plot the null values heatmap for titanicData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice from the above that the known set has missing values in the Age, Cabin and Embarked columns\n",
    "\n",
    "#### 2) Unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data types or relevant info of unknownData \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values in unknownData \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra - optional : Plot the null values heatmap for unknownData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The unknown set has missing values in Age, Cabin and Fare columns**\n",
    "\n",
    "The Embarked feature has only 2 missing values in the train set, which can easily be filled. It will be much more tricky, to deal with the ‘Age’ feature, which has 177 missing values. The ‘Cabin’ feature needs further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and feature creation \n",
    "\n",
    "#### 1. Drop any unnecessary values \n",
    "\n",
    "##### Handling the passenger ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would you deal with PassengerId in titanicData? Come up with a solution. Ensure your changes have gone through \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would you deal with PassengerId in unknownData? Come up with a solution. Ensure your changes have gone through  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Optional --- Feature creation \n",
    "\n",
    "##### Optional --- Extract the titles \n",
    "\n",
    "We could use the Name feature to extract the Titles so that we can build a new feature out of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicData['Title'] = ''\n",
    "unknownData['Title'] = ''\n",
    "\n",
    "for i in titanicData:\n",
    "    titanicData['Title'] = titanicData.Name.str.extract('([A-Za-z]+)\\.') \n",
    "\n",
    "for i in unknownData:\n",
    "    unknownData['Title'] = unknownData.Name.str.extract('([A-Za-z]+)\\.')\n",
    "    \n",
    "titanicData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: execute pd.crosstab() on the combination of titanicData['Title'] and titanicData['Sex'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace many titles with a more common name or classify them as a new value 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [titanicData, unknownData]:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "titanicData[['Title', 'Survived']].groupby(['Title'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Optional --- Work with the Cabin data\n",
    "\n",
    "As a first thought, we can delete the ‘Cabin’ feature but there's something rather interesting. A cabin number looks like ‘C123’ and the letter refers to the deck. Therefore we could extract these and create a new feature, that contains a person's deck. Afterwards, we will convert the feature into a numeric variable. The missing values will be converted to zero. The actual decks of the titanic are ranging from A to G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "data = [titanicData, unknownData]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].fillna(\"X\")\n",
    "    dataset['Deck']  = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset['Deck']  = dataset['Deck'].map(deck)\n",
    "    dataset['Deck']  = dataset['Deck'].fillna(0)\n",
    "    dataset['Deck']  = dataset['Deck'].astype(int)\n",
    "\n",
    "titanicData.Deck.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Optional --- Combine SibSp and Parch\n",
    "\n",
    "SibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. We can create it as follows and also create a feature that shows if someone is not alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [titanicData, unknownData]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "titanicData[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).count().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "titanicData[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Handling (Dropping) the Name,  Ticket and Cabin columns if NOT used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features ['Name', 'Ticket','Cabin'] of your titanicData - set the axis accordingly - inplace or with replacement. \n",
    "# Preview the first 2 rows of titanicData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features ['Name', 'Ticket','Cabin'] of your unknownData - set the axis accordingly - inplace or with replacement. \n",
    "# Preview the first 2 rows of unknownData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most common step across all Supervised Machine Learning models in Python\n",
    "# The first thing you have to ask is what is the label (dependent) variable/column? \n",
    "\n",
    "# Store the feature data from titanicData into a new variable named \"X\" - Extract all columns **except** \n",
    "# from the label column using either indexing (.loc / .iloc) or, easier, .drop()\n",
    "\n",
    "# Store the target data (label/class column) from titanicData into a new variable named \"y\"\n",
    "# Get only the label (class) from titanicData \n",
    "\n",
    "# Print the dimensions (using \"shape\") for both X and y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the entries of y. Do you need to apply LabelEncoding on y? What do you think?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Check the Survival rate (target variable y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to check the counts of the binary class, use the function .value_counts() on y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sns.countplot() function from seaborn to plot the feature \"Survived\" from titanicData\n",
    "# Optional: set also the hue to \"Survived\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that not many passengers survived the accident.\n",
    "\n",
    "Out of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "\n",
    "As mentioned previously, the test (unknown) file provided is used for competition submission. \n",
    "\n",
    "So, we will use the sklearn function to split the known data in two datasets. This is important, so we don't overfit our model. Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. It's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the answers. We will use sklearn's train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split() function from sklearn. Use 30% of your data for the test set. Use stratification if needed. \n",
    "# Set random_state=1 (for reproducibility). Print the dimensionality (shape) of X_train, X_test, y_train, y_test \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing \n",
    "\n",
    "In the case of missing values, we should take care to replace them in the dataset as they prevent machine learning algorithms to run. There are many functions such as `.fillna()` and `.SimpleImputer` from `sklearn` as well as **many strategies** to help with missing data and they depend on whether the missing data is **numeric** or **categorical**.  \n",
    "\n",
    "What strategy is best for you problem very much depends on the specifics of your dataset. However, generally speaking it is not worth to remove large chunks of data.\n",
    "\n",
    "- simply removing rows where there is missing data (e.g. `dropna()` can achieve this)\n",
    "- imputing the values with a summary statistic such as mean or median or most frequent value (e.g. `.fillna()` from `pandas` or `.SimpleImputer()` from `sklearn` module)\n",
    "- replace the values with a reasonable estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly print the NAs per dataset \n",
    "\n",
    "print(\"Nulls in X_train\",\"-\"*20)\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "print(\"Nulls in X_test\",\"-\"*20)\n",
    "print(X_test.isnull().sum())\n",
    "\n",
    "print(\"Nulls in unknownData\",\"-\"*20)\n",
    "print(unknownData.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need one Imputer for the numerical features and one for the categorical features \n",
    "# 1. Instantiate the SimpleImputer for the numerical features using as strategy the 'median'. Assign to a variable named 'imp_num'\n",
    "# 2. Instantiate the SimpleImputer for the categorical features using as strategy the 'most_frequent'. Assign to a variable named 'imp_cat' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute your numerical data using the imp_num Imputer : \n",
    "\n",
    "# 1. .fit_tranform() your imp_num Imputer **ONLY** on the numerical columns of X_train that contain NAs (X_train[['Age', 'Fare']]). \n",
    "# Remember: we only fit() on the trainData but transform() all the DataFrames after the learning process. \n",
    "# Assign back only to X_train[['Age', 'Fare']] \n",
    "# 2. .transform() the numerical columns of X_test that contain NAs (X_test[['Age', 'Fare']]). Assign back only to X_test[['Age', 'Fare']] \n",
    "# 3. .transform() the numerical columns of unknownData that contain NAs (unknownData[['Age', 'Fare']]). Assign back only to unknownData[['Age', 'Fare']]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute your categorical data using the imp_cat Imputer: \n",
    "\n",
    "# 1. .fit_tranform() your imp_cat Imputer ONLY on the categorical columns of X_train that contain NAs (X_train[['Embarked']]). \n",
    "# Remember: we only fit() on the trainData but transform() all the DataFrames after the learning process.\n",
    "# Assign back only to X_train[['Embarked']] \n",
    "# 2. .transform() the numerical columns of X_test that contain NAs (X_test[['Embarked']]). Assign back only to X_test[['Embarked']]  \n",
    "# 3. .transform() the numerical columns of unknownData that contain NAs (unknownData[['Embarked']]). Assign back only to unknownData[['Embarked']]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print once more the NA count as above. Did your changes go through? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the categorical features with One-Hot-Encoding (OHE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the X_train data types with .dtypes once more to detect the categorical columns \n",
    "# Which are the categorical features in this case? You should detect three of them after our feature engineering/creation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot-encoding (OHE) to the categorical features of X_train using pd.get_dummies: \n",
    "\n",
    "# 1. Create a new variable \"X_train_ohe\" and assign the subset of X_train that contains only the (three) categorical features that need to be encoded\n",
    "# 2. As a second step, apply pd.get_dummies(X_train_ohe, dtype=int) and assign back to the variable 'X_train_ohe' to overwrite the entries of the dataframe.  \n",
    "# 3. Remove (drop) the (three) categorical features from X_train (you may need to set the axis accordingly!). Drop with inplace=True OR with assignment back to X_train\n",
    "# 4. Use .join() on X_train with X_train_ohe (contains the result of one hot encoding). Assign back to X_train to overwrite the entries. \n",
    "# Preview the first few rows of X_train. Did your changes go through? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, apply one-hot-encoding (OHE) to the categorical features of *X_test* using pd.get_dummies: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, apply one-hot-encoding (OHE) to the categorical features of *unknownData* using pd.get_dummies: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Check for correlations in the data - Pearson Correlation Heatmap\n",
    "\n",
    "Let's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n",
    "\n",
    "First, we will use the method [`corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) on a `DataFrame` that calculates the correlation between each pair of features. Then, we pass the resulting *correlation matrix* to [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from `seaborn`, which renders a color-coded matrix for the provided values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix of X_train DataFrame using the .corr() function and \n",
    "# save it in a variable called 'corr_matrix'. Then pass the corr_matrix to the sns.heatmap() function for plotting. \n",
    "# Optional: you can use f, ax = plt.subplots(figsize=(12, 8))  \n",
    "# Optional sns.heatmap arguments : annot=True,  annot_kws={'size': 8} and cmap=\"Spectral_r\" \n",
    "\n",
    "########## FILL IN YOUR SOLUTION HERE ########## \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distributions\n",
    "\n",
    "#### What are the ranges of the various features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the .describe() function on the X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the X_train data in a seaborn boxplot. Optional: set the plt.figure(figsize=(15,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Most Machine Learning algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X_train to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the StandardScaler() or MinMaxScaler(). Store into a variable named \"scaler\" \n",
    "\n",
    "# 2. Fit the scaler ONLY on the X_train data - Use the scaler.fit_transform() on the X_train set \n",
    "# You will also need to convert the results of scaler.fit_transform() back to a pandas DataFrame by calling pd.DataFrame() with\n",
    "# columns=X_train.columns. Assign the result back to the variable X_train. \n",
    "\n",
    "# 3. Transform (do not fit!) the test dataset using the (fitted) scaler. Use the scaler.transform() on the X_test. \n",
    "# You will also need to convert the results of scaler.fit_transform() back to a pandas DataFrame by calling pd.DataFrame() with\n",
    "# columns=X_test.columns. Assign the result back to the variable X_test.  \n",
    "\n",
    "# 4. Transform (do not fit!) the test dataset using the (fitted) scaler. Use the scaler.transform() on the unknownData. \n",
    "# You will also need to convert the results of scaler.fit_transform() back to a pandas DataFrame by calling pd.DataFrame() with\n",
    "# columns=unknownData.columns. Assign the result back to the variable unknownData. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once more the boxplot on the scaled X_train data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans clustering on the results of PCA \n",
    "\n",
    "#### PCA \n",
    "\n",
    "In scikit-learn, PCA is implemented as a transformer object that learns `n` components in its `fit()` method, and can be used on new data to project it on these components. More information on how to use the `pca()` function and its parameters can be found at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instantiate the PCA() object by passing as argument the value 0.90 (the cut-off variance value we would like to reach) \n",
    "# and store in a new variable \"pca\"\n",
    "# 2) Apply pca.fit_transform() on the X_train data and store into a new variable named \"pc_scores\"\n",
    "# 3) Print the PCA cummulative variance \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature name for each Principal Component (such as PC1, PC2, ...) up to the detected number of current dimensions\n",
    "\n",
    "PCs = ['PC'+str(i+1) for i in range(pc_scores.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pc_scores to a pandas DataFrame using pd.DataFrame() and pass as the columns argument the PCs list that was created in the prevous step\n",
    "# Assign back to pc_scores. Preview the first entries of pc_scores \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 Principal Components (\"PC1\" vs. \"PC2\") in a seaborn scatterplot \n",
    "# Optional/extra: Should you wish, you can join the y_train class label in the pc_scores and use it as hue in the scatterplot \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. K-means\n",
    "\n",
    "#### Elbow method \n",
    "\n",
    "Elbow Method : There’s a sweet spot where the SSE curve starts to bend known as the elbow point. The x-value of this point is thought to be a reasonable trade-off between error and number of clusters. The elbowpoint is the point where the rate of decrease of mean distance i.e. SSE will not change significantly with increase in number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you run the elbow rule to find the optimal number of K for kmeans that we need to use on our pc_scores?? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instantiate a KMeans object with n_clusters equal to the **OPTIMAL k** as found above and store it in a variable named \"kmodel\" \n",
    "# 2) .fit() the kmodel model on the pc_scores(!!) data \n",
    "# 3) Use the .labels_ parameter on kmodel to get the assinged clusters and save the results in a new variable \"cluster_assignment\".\n",
    "# 4) Save the output of cluster_assignment to pc_scores[\"KMCluster\"]  \n",
    "# 5) Preview the first few rows of pc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2 Principal Components (PC scores) in a scatterplot using the \"KMCluster\" as hue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, predict and solve\n",
    "\n",
    "### Classifiers - Predict using the *subset* test set (not the unknown yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store and compare all evaluation results (performance metrics) across all classifiers \n",
    "\n",
    "clf_results = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Support Vector Machine (RBF) - benchmark model (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the SVC() classifier using the default parameters (Default hyperparameters are C=1.0, kernel=rbf and gamma=auto) \n",
    "# Assign the result into a new variable named \"rbf_svm\" \n",
    "# Step 2 - Fit the rbf_svm model to the training set (X_train, y_train)\n",
    "# Step 3 - Predict the test data (X_test) using the rbf_svm model and assign to \"y_pred_rbf_svm\" \n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the benchmark SVM classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Benchmark RBF SVM', \n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_rbf_svm), 3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_rbf_svm), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Linear SVM - benchmark model (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the SVC classifier with a linear kernel. Assign the result into a new variable named \"linear_svm\" \n",
    "# Step 2 - Fit the linear_svm model to the training set (X_train, y_train)\n",
    "# Step 3 - Predict the test data (X_test) using the linear_svm model and assign to \"y_pred_linear_svm\"\n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the benchmark SVM classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Benchmark Linear SVM', \n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_linear_svm), 3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_linear_svm), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Optimal SVM - SVM hyperparameter tuning\n",
    "\n",
    "Proper choice of C and gamma is critical to the SVM’s performance. **One is advised to use GridSearchCV with `C` and `gamma` spaced exponentially far apart to choose good values.** Detailed information on the SVM hyperparameters can be found at https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py \n",
    "\n",
    "As a first step, create a dictionary of hyperparameter ranges and conduct a grid or random search with cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV() with 5-fold or 10-fold cross-validation (cv=5 or cv=10) on the SVC() classifier \n",
    "# (more cv folds reduces the chances of overfitting but also increases the run time) \n",
    "# 1. Create the dictionary of hyperparameters for the SVM (SVC classifier) \n",
    "# 2. Set up the GridSearchCV and assign to a new variable named svm_grid_cv. \n",
    "# Optional: You can experiment with the scoring options (you need to check the documentation)\n",
    "# 3. Fit svm_grid_cv to X_train and y_train \n",
    "# 4. Report the optimal parameters using 'cv_svm.best_params_'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the (optimal) classifier using the optimal parameters detected by the tuning process. Assign to a new variable svm_opt. \n",
    "# Note: you can use either svm_grid_cv.best_estimator_ to retrieve the model or the svm_grid_cv.best_params_ from above into a new SVC()\n",
    "# 2 - Fit the svm_opt model to the training set (X_train)\n",
    "# 3 - Predict the test data (X_test) using the svm_opt model. Assign to a variable named y_pred_svm_opt\n",
    "# 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the optimal SVM classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Tuned SVM', \n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_svm_opt), 3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_svm_opt), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. KNN - benchmark model (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the KNeighborsClassifier() classifier using the default parameters  \n",
    "# Assign the result into a new variable named \"knn\" \n",
    "# Step 2 - Fit the knn model to the training set (X_train, y_train)\n",
    "# Step 3 - Predict the test data (X_test) using the knn model. Assign to y_pred_knn\n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV() with 5-fold or 10-fold cross-validation (cv=5 or cv=10) on KNeighborsClassifier() \n",
    "# (more cv folds reduces the chances of overfitting but also increases the run time) on the KNeighborsClassifier classifier \n",
    "# 1. Create the dictionary of hyperparameters for KNN\n",
    "# 2. Set up the GridSearchCV and assign to a new variable named knn_grid_cv\n",
    "# 3. Fit the grid or random search model to X_train and y_train \n",
    "# 4. Report the optimal parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by the tuning process\n",
    "# Note: you can use either knn_grid_cv.best_estimator_ to retrieve the optimal model. Assign to knn_opt.  \n",
    "# 2 - Fit the knn_opt model to the training set (X_train, y_train)\n",
    "# 3 - Predict the test data (X_test) using the knn_opt model. Assign to a variable named y_pred_knn_opt\n",
    "# 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the optimal KNN classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Tuned KNN',\n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_knn_opt),3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_knn_opt), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Random Forest - benchmark model (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the RandomForestClassifier(random_state=0) classifier using the default parameters  \n",
    "# Assign the result into a new variable named \"rf\" \n",
    "# Step 2 - Fit the rf model to the training set (X_train, y_train)\n",
    "# Step 3 - Predict the test data (X_test) using the rf model. Assign to y_pred_rf \n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Benchmark RF',\n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_rf),3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_rf), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Optimal RF - tune the RF hyperparameters with RandomizedSearchCV( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV() with 5-fold or 10-fold cross-validation (cv=5 or cv=10)\n",
    "# (more cv folds reduces the chances of overfitting but also increases the run time) on the RandomForestClassifier(random_state=0) classifier \n",
    "# 1. Create the dictionary of hyperparameters for RF\n",
    "# 2. Set up the **RandomSearchCV**  and assign to a new variable named cv_rf\n",
    "# 3. Fit cv_rf to X_train and y_train \n",
    "# 4. Report the optimal parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by the tuning process.\n",
    "# Note: you can use either cv_rf.best_estimator_ to retrieve the optimal model. Assign to rf_opt.  \n",
    "# 2 - Fit the rf_opt model to the training set (X_train, y_train)\n",
    "# 3 - Predict the test data (X_test) using the rf_opt model. Assign to a variable named y_pred_rf_opt\n",
    "# 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the optimal KNN classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Tuned Random Forest',\n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_rf_opt),3),  \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_rf_opt), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance from the rf classifier using rf_opt.feature_importances_\n",
    "# Cast it into a pd.DataFrame and use sort_values to sort by the importance \n",
    "# Plot the rf_opt.feature_importances_ in a barplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Decision Tree - benchmark model (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the DecisionTreeClassifier() classifier using the default parameters  \n",
    "# Assign the result into a new variable named \"dt\" \n",
    "# Step 2 - Fit the dt model to the training set (X_train, y_train)\n",
    "# Step 3 - Predict the test data (X_test) using the dt model. Assign to y_pred_dt\n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (time-dependent): YOU CAN TUNE THE PARAMETERS OF DT TO FIND THE OPTIMAL MODEL #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the optimal KNN classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Benchmark Decision Tree',\n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_dt),3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_dt), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_results.sort_values(by='F1', ascending=False, inplace=True)\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (OPTIONAL) Other models: Logistic Regression - benchmark model (default parameters) \n",
    "\n",
    "Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the LogisticRegression() classifier using the default parameters  \n",
    "# Assign the result into a new variable named \"logreg\" \n",
    "# Step 2 - Fit the logreg model to the training set (X_train)\n",
    "# Step 3 - Predict the test data (X_test) using the logreg model \n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print('Test set accuracy: ', round(metrics.accuracy_score(y_test, y_pred_logreg), 3))\n",
    "print('\\n', metrics.classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOU CAN TUNE THE PARAMETERS OF LOGISTIC REGRESSION TO FIND THE OPTIMAL MODEL #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the optimal KNN classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'LogisticRegression',\n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_logreg),3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_logreg), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (OPTIONAL) Other models: ExtraTreesClassifier - benchmark model (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the ExtraTreesClassifier() classifier using the default parameters  \n",
    "# Assign the result into a new variable named \"extc\" \n",
    "# Step 2 - Fit the extc model to the training set (X_train)\n",
    "# Step 3 - Predict the test data (X_test) using the extc model \n",
    "# Step 4 - Print the final overall accuracy and classification_report for the test set (X_test) \n",
    "\n",
    "extc = ExtraTreesClassifier()\n",
    "extc.fit(X_train, y_train)\n",
    "y_pred_extc = extc.predict(X_test)\n",
    "\n",
    "print('Test set accuracy: ', round(metrics.accuracy_score(y_test, y_pred_extc), 3))\n",
    "print('\\n', metrics.classification_report(y_test, y_pred_extc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "                  \"max_features\": [1, 3, 10],\n",
    "                  \"min_samples_split\": [2, 3, 10],\n",
    "                  \"min_samples_leaf\": [1, 3, 10],\n",
    "                  \"bootstrap\": [False],\n",
    "                  \"n_estimators\" :[100,300],\n",
    "                  \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "cv_extc = RandomizedSearchCV(ExtraTreesClassifier(random_state=0), \n",
    "                           param_distributions=ex_param_grid, \n",
    "                           n_iter = 50,\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1,\n",
    "                           random_state=0)\n",
    "\n",
    "cv_extc.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters using grid search: \\n', cv_extc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by the tuning process\n",
    "\n",
    "extc_opt = cv_extc.best_estimator_\n",
    "extc_opt.fit(X_train, y_train) \n",
    "y_pred_extc_opt = extc_opt.predict(X_test)\n",
    "\n",
    "print('Test set accuracy: ', round(metrics.accuracy_score(y_test, y_pred_extc_opt), 3))\n",
    "print('\\n', metrics.classification_report(y_test, y_pred_extc_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance from the extc_opt classifier using extc_opt.feature_importances_\n",
    "# Cast it into a pd.DataFrame and use sort_values to sort by the importance \n",
    "\n",
    "feature_scores = pd.DataFrame(extc_opt.feature_importances_, \n",
    "                              index=X_train.columns, \n",
    "                              columns=['Importance'])\n",
    "feature_scores.sort_values(by='Importance', ascending=False, inplace=True) \n",
    "\n",
    "# Plot the rf_opt.feature_importances_ in a barplot \n",
    "f, ax = plt.subplots(figsize=(30, 20))\n",
    "ax = sns.barplot(x='Importance', y=feature_scores.index, data=feature_scores)\n",
    "ax.set_title(\"Extra Trees feature importance\", size = 20)\n",
    "ax.set_yticklabels(feature_scores.index, size = 20)\n",
    "ax.set_xlabel(\"Feature importance score\", size = 20)\n",
    "ax.set_ylabel(\"Features\", size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the optimal Extra Trees classifier as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Extra Trees',\n",
    "                                                         'Accuracy': round(metrics.accuracy_score(y_test, y_pred_extc_opt),3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_extc_opt), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (OPTIONAL) Other models: Ensemble with Voting (heterogeneous models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Build a Voting Ensemble model with some heterogeneous models with pre-defined parameters \n",
    "# Alternatively, you can use the combination of optimized (tuned) models \n",
    "\n",
    "ensemble =VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n",
    "                                      ('RBF', SVC(probability=True, kernel='rbf', C=0.5,gamma=0.1)),\n",
    "                                      ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n",
    "                                      ('LR',  LogisticRegression(C=0.05)),\n",
    "                                      ('DT',  DecisionTreeClassifier(random_state=0)),\n",
    "                                      ('NB',  GaussianNB()),\n",
    "                                      ('svm', SVC(kernel='rbf',probability=True))\n",
    "                                     ], \n",
    "                       voting='soft')\n",
    "\n",
    "ensemble.fit(X_train, y_train) \n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "print('The accuracy for ensembled model is:', ensemble.score(X_test, y_test))\n",
    "print('\\n', metrics.classification_report(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOU CAN TUNE THE PARAMETERS OF THE VOTING ENSEMBLE TO FIND THE OPTIMAL MODEL \n",
    "# OR COMBINE THE OPTIMAL MODELS FROM THE VARIOUS TUNING PROCESSES #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results of the Voting Ensemble model (heterogeneous) as a new row into clf_results that collects all the results across all models\n",
    "\n",
    "clf_results = pd.concat([clf_results, pd.json_normalize({'Model': 'Voting Ensemble model (heterogeneous)',\n",
    "                                                         'Accuracy': round(ensemble.score(X_test, y_test),3), \n",
    "                                                         'F1': round(metrics.f1_score(y_test, y_pred_ensemble), 3)})])\n",
    "\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare once more all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_results.sort_values(by='F1', ascending=False, inplace=True)\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers - Predict the unknown test set\n",
    "\n",
    "Using the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case 1: use a single optimal (tuned) model \n",
    "\n",
    "Let's assume we want to use the optimal Random Forest to make the predictions on our final test (unknown) dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Can you use any optimal model to predict the classes of your unknownData? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the distribution of your y_pred to investigate how your model did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv file \n",
    "# output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
